{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "def get_custom_gpu_names():\n",
    "    # check if ray is initialized\n",
    "    if not ray.is_initialized():\n",
    "        ray.init(address=\"auto\", ignore_reinit_error=True)\n",
    "    # Get all resources in the Ray cluster\n",
    "    resources = ray.cluster_resources()\n",
    "    gpu_names = [k for k in resources.keys() if \"_GPU\" in k]\n",
    "    # sort alphabetically\n",
    "    gpu_names.sort()\n",
    "    return gpu_names\n",
    "\n",
    "def select_gpu():\n",
    "    assigned_resources = ray.get_runtime_context().get_assigned_resources()\n",
    "    resource_name = [k for k in assigned_resources.keys() if \"_GPU\" in k][0]  # e.g., \"node1_GPU0\"\n",
    "    local_gpu_index = int(resource_name.split(\"_GPU\")[-1])       # Extract \"0\"    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(local_gpu_index)\n",
    "\n",
    "@ray.remote\n",
    "class CustomGPU:\n",
    "    def __init__(self):\n",
    "        select_gpu()\n",
    "\n",
    "    def get_free_memory(self):\n",
    "        free = torch.cuda.mem_get_info(0)[0] / 1024 / 1024 / 1024 # GB\n",
    "        self.free_memory = free\n",
    "        return free\n",
    "\n",
    "def find_top_k_gpu(k=1):\n",
    "    print(f\"Finding top {k} GPU...\")\n",
    "    # check if ray is initialized\n",
    "    if not ray.is_initialized():\n",
    "        ray.init(address=\"auto\", ignore_reinit_error=True)\n",
    "    gpu_names = get_custom_gpu_names()\n",
    "    gpu_free_memory = []\n",
    "    for gpu_name in gpu_names:\n",
    "        actor = CustomGPU.options(resources={gpu_name: 0.01}).remote()\n",
    "        free_memory = ray.get(actor.get_free_memory.remote())\n",
    "        gpu_free_memory.append((gpu_name, free_memory))\n",
    "        print(f\"GPU: {gpu_name}, Free memory: {free_memory:.2f} GB\")\n",
    "        ray.kill(actor)\n",
    "    # sort by free memory\n",
    "    gpu_free_memory.sort(key=lambda x: x[1], reverse=True)\n",
    "    gpu_names = [gpu_name for gpu_name, _ in gpu_free_memory]\n",
    "    if k == 0:\n",
    "        return gpu_names\n",
    "    top_k_gpu = gpu_names[:k]\n",
    "    return top_k_gpu\n",
    "\n",
    "def find_eligible_gpu(gpu_names, n_gpu=4, free_memory_threshold=1):\n",
    "    # find all GPUs with free memory greater than the threshold in unit of GB\n",
    "    eligible_gpu = []\n",
    "    for gpu_name in gpu_names:\n",
    "        actor = CustomGPU.options(resources={gpu_name: 0.01}).remote()\n",
    "        free_memory = ray.get(actor.get_free_memory.remote())\n",
    "        if free_memory > free_memory_threshold:\n",
    "            eligible_gpu.append(gpu_name)\n",
    "            print(f\"Found eligible GPU: {gpu_name}, Free memory: {free_memory:.2f} GB\")\n",
    "        if len(eligible_gpu) >= n_gpu:\n",
    "            return eligible_gpu\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_task():\n",
    "    current_time = time.localtime()\n",
    "    print(f'start task at {current_time.tm_hour:02d}:{current_time.tm_min:02d}:{current_time.tm_sec:02d}')\n",
    "    time.sleep(5)\n",
    "    a = torch.randn(1000, 1000)\n",
    "    b = torch.randn(1000, 1000)\n",
    "    c = torch.matmul(a, b)\n",
    "    result = c.shape\n",
    "    current_time = time.localtime()\n",
    "    print(f'end task at {current_time.tm_hour:02d}:{current_time.tm_min:02d}:{current_time.tm_sec:02d}')\n",
    "    return result\n",
    "\n",
    "@ray.remote\n",
    "class worker:\n",
    "    def __init__(self):\n",
    "        select_gpu()\n",
    "\n",
    "    def task(self):\n",
    "        return my_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # sort GPUs by free memory\n",
    "    sorted_gpu_names = find_top_k_gpu(k=0)\n",
    "    # Run 4 workers on top 4 GPUs simultaneously\n",
    "    n_workers = 4\n",
    "    free_memory_threshold = 1 # GB\n",
    "    eligible_gpu = find_eligible_gpu(sorted_gpu_names, n_gpu=n_workers, free_memory_threshold=free_memory_threshold)\n",
    "    print(f\"Eligible GPUs: {eligible_gpu}\")\n",
    "    workers = [worker.options(resources={gpu: 0.01}).remote() for gpu in eligible_gpu]\n",
    "    results = ray.get([worker.task.remote() for worker in workers])\n",
    "    print(results)\n",
    "    # shutdown ray\n",
    "    ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
