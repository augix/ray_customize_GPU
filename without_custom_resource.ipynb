{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 13:46:15,959\tINFO worker.py:1654 -- Connecting to existing Ray cluster at address: 10.11.140.31:6379...\n",
      "2025-03-18 13:46:15,977\tINFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding 4 GPUs with free memory greater than 10 GB\n",
      "\u001b[36m(CustomGPU pid=7290)\u001b[0m selecting 10.11.140.31_GPU0\n",
      "Found eligible GPU: 10.11.140.31_GPU0, Free memory: 23.36 GB\n",
      "Found eligible GPU: 10.11.140.31_GPU1, Free memory: 23.42 GB\n",
      "Found eligible GPU: 10.11.140.64_GPU0, Free memory: 17.82 GB\n",
      "\u001b[36m(CustomGPU pid=1877866, ip=10.11.140.64)\u001b[0m selecting 10.11.140.64_GPU1\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "Found eligible GPU: 10.11.140.64_GPU1, Free memory: 23.43 GB\n",
      "Eligible GPUs: ['10.11.140.31_GPU0', '10.11.140.31_GPU1', '10.11.140.64_GPU0', '10.11.140.64_GPU1']\n",
      "\u001b[36m(worker pid=7295)\u001b[0m start task at 13:46:26\n",
      "\u001b[36m(worker pid=7295)\u001b[0m end task at 13:46:32\n",
      "\u001b[36m(worker pid=1877955, ip=10.11.140.64)\u001b[0m selecting 10.11.140.64_GPU1\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(worker pid=1877955, ip=10.11.140.64)\u001b[0m start task at 13:46:27\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "[torch.Size([1000, 1000]), torch.Size([1000, 1000]), torch.Size([1000, 1000]), torch.Size([1000, 1000])]\n",
      "\u001b[36m(worker pid=1877955, ip=10.11.140.64)\u001b[0m end task at 13:46:32\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "def get_gpu_names():\n",
    "    gpu_names = []\n",
    "    if not ray.is_initialized():\n",
    "        ray.init(address='auto', ignore_reinit_error=True)\n",
    "    nodes = ray.nodes()\n",
    "    for node in nodes:\n",
    "        node_name = node['NodeName']\n",
    "        num_gpus = node.get('Resources', {}).get('GPU', 0)\n",
    "        num_gpus = int(num_gpus)\n",
    "        if num_gpus > 0:\n",
    "            for i in range(num_gpus):\n",
    "                gpu_name = f\"{node_name}_GPU{i}\"\n",
    "                gpu_names.append(gpu_name)\n",
    "    return gpu_names\n",
    "\n",
    "def get_custom_gpu_names():\n",
    "    # check if ray is initialized\n",
    "    if not ray.is_initialized():\n",
    "        ray.init(address='auto', ignore_reinit_error=True)\n",
    "    # Get all resources in the Ray cluster\n",
    "    resources = ray.cluster_resources()\n",
    "    gpu_names = [k for k in resources.keys() if \"_GPU\" in k]\n",
    "    # sort alphabetically\n",
    "    gpu_names.sort()\n",
    "    return gpu_names\n",
    "\n",
    "def select_gpu(gpu_name):\n",
    "    # assigned_resources = ray.get_runtime_context().get_assigned_resources()\n",
    "    # resource_name = [k for k in assigned_resources.keys() if \"_GPU\" in k][0]  # e.g., \"node1_GPU0\"\n",
    "    print(f'selecting {gpu_name}')\n",
    "    local_gpu_index = int(gpu_name.split(\"_GPU\")[-1])       # Extract \"0\"    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(local_gpu_index)\n",
    "\n",
    "@ray.remote\n",
    "class CustomGPU:\n",
    "    def __init__(self, gpu_name):\n",
    "        select_gpu(gpu_name)\n",
    "\n",
    "    def get_free_memory(self):\n",
    "        free = torch.cuda.mem_get_info(0)[0] / 1024 / 1024 / 1024 # GB\n",
    "        self.free_memory = free\n",
    "        return free\n",
    "\n",
    "def find_top_k_gpu(k=1):\n",
    "    print(f\"Finding top {k} GPU...\")\n",
    "    # check if ray is initialized\n",
    "    if not ray.is_initialized():\n",
    "        ray.init(address='auto', ignore_reinit_error=True)\n",
    "    # gpu_names = get_custom_gpu_names()\n",
    "    gpu_names = get_gpu_names()\n",
    "    print('all GPUs:', gpu_names)\n",
    "    gpu_free_memory = []\n",
    "    for gpu_name in gpu_names:\n",
    "        try:\n",
    "            node_name = gpu_name.split('_GPU')[0]\n",
    "            # actor = CustomGPU.options(resources={gpu_name: 0.01}).remote()\n",
    "            actor = CustomGPU.options(resources={f\"node:{node_name}\": 0.01}, num_cpus=1).remote(gpu_name)\n",
    "            free_memory = ray.get(actor.get_free_memory.remote())\n",
    "            gpu_free_memory.append((gpu_name, free_memory))\n",
    "            print(f\"GPU: {gpu_name}, Free memory: {free_memory:.2f} GB\")\n",
    "            ray.kill(actor)\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking {gpu_name}: {e}\")\n",
    "    # sort by free memory\n",
    "    gpu_free_memory.sort(key=lambda x: x[1], reverse=True)\n",
    "    gpu_names = [gpu_name for gpu_name, _ in gpu_free_memory]\n",
    "    if k == 0:\n",
    "        return gpu_names\n",
    "    top_k_gpu = gpu_names[:k]\n",
    "    return top_k_gpu\n",
    "\n",
    "def find_eligible_gpu(gpu_names, n_gpu=4, free_memory_threshold=10):\n",
    "    print(f'finding {n_gpu} GPUs with free memory greater than {free_memory_threshold} GB')\n",
    "    # find all GPUs with free memory greater than the threshold in unit of GB\n",
    "    eligible_gpu = []\n",
    "    for gpu_name in gpu_names:\n",
    "        try:\n",
    "            node_name = gpu_name.split('_GPU')[0]\n",
    "            # actor = CustomGPU.options(resources={gpu_name: 0.01}).remote()\n",
    "            actor = CustomGPU.options(resources={f\"node:{node_name}\": 0.01}, num_cpus=1).remote(gpu_name)\n",
    "            free_memory = ray.get(actor.get_free_memory.remote())\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking {gpu_name}: {e}\")\n",
    "            continue\n",
    "        if free_memory > free_memory_threshold:\n",
    "            eligible_gpu.append(gpu_name)\n",
    "            print(f\"Found eligible GPU: {gpu_name}, Free memory: {free_memory:.2f} GB\")\n",
    "        if len(eligible_gpu) >= n_gpu:\n",
    "            return eligible_gpu\n",
    "    return None\n",
    "\n",
    "def my_task():\n",
    "    current_time = time.localtime()\n",
    "    print(f'start task at {current_time.tm_hour:02d}:{current_time.tm_min:02d}:{current_time.tm_sec:02d}')\n",
    "    time.sleep(5)\n",
    "    a = torch.randn(1000, 1000)\n",
    "    b = torch.randn(1000, 1000)\n",
    "    c = torch.matmul(a, b)\n",
    "    result = c.shape\n",
    "    current_time = time.localtime()\n",
    "    print(f'end task at {current_time.tm_hour:02d}:{current_time.tm_min:02d}:{current_time.tm_sec:02d}')\n",
    "    return result\n",
    "\n",
    "@ray.remote\n",
    "class worker:\n",
    "    def __init__(self, gpu_name):\n",
    "        select_gpu(gpu_name)\n",
    "\n",
    "    def task(self):\n",
    "        return my_task()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ray_address = '10.15.0.16:6379'\n",
    "    # ray.init(address=ray_address, ignore_reinit_error=True)\n",
    "    # sort GPUs by free memory\n",
    "    # sorted_gpu_names = find_top_k_gpu(k=0)\n",
    "\n",
    "    # Run 4 workers on top 4 GPUs simultaneously\n",
    "    n_workers = 4\n",
    "    free_memory_threshold=10 # GB\n",
    "    gpu_names = get_gpu_names()\n",
    "    eligible_gpu = find_eligible_gpu(gpu_names, n_gpu=n_workers, free_memory_threshold=free_memory_threshold)\n",
    "    print(f\"Eligible GPUs: {eligible_gpu}\")\n",
    "    if len(eligible_gpu) == n_workers:\n",
    "        node_names = [gpu.split('_GPU')[0] for gpu in eligible_gpu]\n",
    "        workers = [worker.options(resources={f\"node:{node_name}\": 0.01}).remote(gpu) for gpu, node_name in zip(eligible_gpu, node_names)]\n",
    "        results = ray.get([worker.task.remote() for worker in workers])\n",
    "        print(results)\n",
    "    # shutdown ray\n",
    "    ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
